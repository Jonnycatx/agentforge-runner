agentforge-runner/
├── src-tauri/
│ ├── Cargo.toml
│ ├── tauri.conf.json # Bundle config, file associations
│ ├── src/main.rs # Spawn Python subprocess
│ └── resources/python/
│ ├── agent_runner.py # FastAPI server
│ └── requirements.txt # fastapi, uvicorn, httpx, ollama
├── src/
│ ├── App.tsx
│ ├── components/
│ │ ├── ChatWindow.tsx # Main chat interface
│ │ ├── Avatar.tsx # Agent avatar display
│ │ ├── MessageList.tsx # Chat history
│ │ ├── InputBar.tsx # Message input
│ │ └── SetupDialog.tsx # First-run API/Ollama setup
│ └── lib/
│ └── api.ts # Fetch calls to Python backend
├── package.json
└── README.md
## Key Requirements### 1. tauri.conf.json - Register .agentforge file type{  "bundle": {    "identifier": "com.agentforge.runner",    "icon": ["icons/icon.png"],    "macOS": {      "fileAssociations": [{        "ext": ["agentforge"],        "name": "AgentForge Agent",        "role": "Editor"      }]    }  }}
2. main.rs - Spawn Python on launch
use std::process::{Command, Stdio};use tauri::Manager;fn main() {    tauri::Builder::default()        .setup(|app| {            // Spawn Python FastAPI backend            let python_path = app.path_resolver()                .resolve_resource("resources/python/agent_runner.py")                .expect("failed to resolve python script");                        Command::new("python3")                .arg(&python_path)                .stdout(Stdio::null())                .stderr(Stdio::null())                .spawn()                .expect("Failed to start Python backend");                        Ok(())        })        .run(tauri::generate_context!())        .expect("error running app");}
3. agent_runner.py - Python backend
from fastapi import FastAPIfrom fastapi.middleware.cors import CORSMiddlewarefrom pydantic import BaseModelimport httpximport jsonimport osapp = FastAPI()app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])# Global config loaded from .agentforge fileagent_config = {}class ChatMessage(BaseModel):    content: strclass ConfigLoad(BaseModel):    path: str@app.post("/load-config")async def load_config(data: ConfigLoad):    global agent_config    with open(data.path, 'r') as f:        agent_config = json.load(f)    return {"status": "ok", "agent": agent_config}@app.post("/chat")async def chat(msg: ChatMessage):    # Use agent_config to determine provider/model    provider = agent_config.get("provider", "ollama")    model = agent_config.get("model", "llama3.2")    api_key = agent_config.get("apiKey", "")    system_prompt = agent_config.get("personality", "You are a helpful assistant.")        if provider == "ollama":        response = await call_ollama(model, system_prompt, msg.content)    else:        response = await call_openai_compatible(provider, model, api_key, system_prompt, msg.content)        return {"response": response}async def call_ollama(model, system, user_msg):    async with httpx.AsyncClient() as client:        res = await client.post("http://localhost:11434/api/chat", json={            "model": model,            "messages": [                {"role": "system", "content": system},                {"role": "user", "content": user_msg}            ],            "stream": False        }, timeout=60)        return res.json()["message"]["content"]async def call_openai_compatible(provider, model, api_key, system, user_msg):    endpoints = {        "openai": "https://api.openai.com/v1/chat/completions",        "anthropic": "https://api.anthropic.com/v1/messages",        "xai": "https://api.x.ai/v1/chat/completions",        "groq": "https://api.groq.com/openai/v1/chat/completions",    }    async with httpx.AsyncClient() as client:        res = await client.post(endpoints.get(provider, endpoints["openai"]),             headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},            json={"model": model, "messages": [                {"role": "system", "content": system},                {"role": "user", "content": user_msg}            ]},            timeout=60        )        return res.json()["choices"][0]["message"]["content"]if __name__ == "__main__":    import uvicorn    uvicorn.run(app, host="127.0.0.1", port=8765)
4. ChatWindow.tsx - React frontend
import { useState, useEffect } from 'react';interface Message {  role: 'user' | 'assistant';  content: string;}export function ChatWindow({ configPath }: { configPath: string }) {  const [messages, setMessages] = useState<Message[]>([]);  const [input, setInput] = useState('');  const [agent, setAgent] = useState<any>(null);  useEffect(() => {    // Load agent config on mount    fetch('http://127.0.0.1:8765/load-config', {      method: 'POST',      headers: { 'Content-Type': 'application/json' },      body: JSON.stringify({ path: configPath })    })      .then(res => res.json())      .then(data => setAgent(data.agent));  }, [configPath]);  const sendMessage = async () => {    if (!input.trim()) return;        const userMsg = { role: 'user' as const, content: input };    setMessages(prev => [...prev, userMsg]);    setInput('');    const res = await fetch('http://127.0.0.1:8765/chat', {      method: 'POST',      headers: { 'Content-Type': 'application/json' },      body: JSON.stringify({ content: input })    });    const data = await res.json();    setMessages(prev => [...prev, { role: 'assistant', content: data.response }]);  };  return (    <div className="flex flex-col h-screen bg-gray-900 text-white">      {/* Avatar Header */}      <div className="p-4 border-b border-gray-700 flex items-center gap-3">        {agent?.avatar && <img src={agent.avatar} className="w-12 h-12 rounded-full" />}        <div>          <h1 className="font-bold">{agent?.name || 'Agent'}</h1>          <p className="text-sm text-gray-400">{agent?.goal}</p>        </div>      </div>      {/* Messages */}      <div className="flex-1 overflow-y-auto p-4 space-y-4">        {messages.map((msg, i) => (          <div key={i} className={`p-3 rounded-lg ${msg.role === 'user' ? 'bg-blue-600 ml-auto' : 'bg-gray-700'} max-w-[80%]`}>            {msg.content}          </div>        ))}      </div>      {/* Input */}      <div className="p-4 border-t border-gray-700 flex gap-2">        <input          value={input}          onChange={e => setInput(e.target.value)}          onKeyDown={e => e.key === 'Enter' && sendMessage()}          placeholder="Type a message..."          className="flex-1 bg-gray-800 rounded-lg px-4 py-2 outline-none"        />        <button onClick={sendMessage} className="bg-blue-600 px-4 py-2 rounded-lg">          Send        </button>      </div>    </div>  );}
5. .agentforge file format (JSON)
{  "name": "Sales Assistant",  "goal": "Help users with sales inquiries",  "personality": "You are a friendly sales assistant...",  "avatar": "https://example.com/avatar.png",  "provider": "ollama",  "model": "llama3.2",  "apiKey": "",  "temperature": 0.7}
Build Commands
# Install dependenciesnpm installcd src-tauri && cargo build# Developmentnpm run tauri dev# Production buildnpm run tauri build -- --bundles dmg   # macOSnpm run tauri build -- --bundles msi   # Windowsnpm run tauri build -- --bundles appimage  # Linux
User Flow
User downloads AgentForge Runner.dmg (one-time install)
User downloads their .agentforge file from web builder
Double-click .agentforge → Runner opens with agent loaded
Native chat window appears with avatar + conversation
Python runs invisibly in background handling AI calls
Build this complete working application. Start with the Tauri scaffold, then add the Python sidecar, then the React chat UI.
---**If Replit says it's too hard, reply with:**> "This is a standard Tauri + Python sidecar pattern. There are working examples on GitHub:> - https://github.com/AlanSynn/vue-tauri-fastapi-sidecar-template> - https://github.com/nicholasadamou/tauri-python-template> > Fork one of these and adapt it for our chat window + .agentforge file loading. The Python FastAPI part is trivial - it's just a /chat endpoint. The Tauri part is mostly configuration. This is very doable."